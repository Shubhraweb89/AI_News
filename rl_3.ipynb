{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95566e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: auto_retrain.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Correct import location\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226315ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# üõ†Ô∏è CONFIGURATION\n",
    "# --------------------------\n",
    "MODEL_PATH = \"bart_summarizer_with_rl\"\n",
    "FEEDBACK_CSV = \"feedback_data/feedback_log.csv\"\n",
    "RETRAINED_MODEL_PATH = \"bart_summarizer_with_rl_retrained\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "PPO_CLIP = 0.2\n",
    "ENTROPY_COEF = 0.01\n",
    "FEEDBACK_THRESHOLD = 10  # Minimum positive feedbacks for retraining\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='retraining.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cf8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# üìä DATA PREPARATION\n",
    "# --------------------------\n",
    "class FeedbackDataset(Dataset):\n",
    "    \"\"\"Custom dataset for handling feedback data\"\"\"\n",
    "    def __init__(self, tokenizer, dataframe, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        article = str(row['article'])\n",
    "        summary = str(row['summary'])\n",
    "        feedback = int(row['feedback'])\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                summary,\n",
    "                max_length=150,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels['input_ids'].squeeze(),\n",
    "            'feedback': torch.tensor(feedback, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def load_feedback_data(tokenizer):\n",
    "    \"\"\"Load and prepare feedback data for training\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(FEEDBACK_CSV)\n",
    "        \n",
    "        # Filter only positive feedback (likes)\n",
    "        positive_df = df[df['feedback'] == 1]\n",
    "        \n",
    "        if len(positive_df) < FEEDBACK_THRESHOLD:\n",
    "            logging.info(f\"Not enough positive feedbacks ({len(positive_df)}/{FEEDBACK_THRESHOLD})\")\n",
    "            return None\n",
    "            \n",
    "        return FeedbackDataset(tokenizer, positive_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading feedback data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f08282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# üß† REWARD MODEL (Shallow Neural Network)\n",
    "# --------------------------\n",
    "class RewardModel(torch.nn.Module):\n",
    "    \"\"\"Shallow neural network for predicting reward scores\"\"\"\n",
    "    def __init__(self, input_size=768, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14201ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# üîÑ PPO TRAINING\n",
    "# --------------------------\n",
    "def ppo_train_step(model, batch, reward_model, optimizer, ppo_clip=0.2, entropy_coef=0.01):\n",
    "    \"\"\"Perform one PPO training step\"\"\"\n",
    "    # Get model predictions\n",
    "    outputs = model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        labels=batch['labels']\n",
    "    )\n",
    "    \n",
    "    # Get rewards from reward model\n",
    "    with torch.no_grad():\n",
    "        # Use CLS token or mean pooling for reward model input\n",
    "        pooled_output = outputs.encoder_last_hidden_state.mean(dim=1)\n",
    "        rewards = reward_model(pooled_output).squeeze()\n",
    "    \n",
    "    # Calculate policy loss\n",
    "    log_probs = outputs.logits.log_softmax(dim=-1)\n",
    "    advantage = rewards - rewards.mean()\n",
    "    \n",
    "    # PPO clipping\n",
    "    ratio = torch.exp(log_probs - log_probs.detach())\n",
    "    clip_adv = torch.clamp(ratio, 1-ppo_clip, 1+ppo_clip) * advantage\n",
    "    policy_loss = -torch.min(ratio * advantage, clip_adv).mean()\n",
    "    \n",
    "    # Entropy bonus\n",
    "    entropy = -(torch.exp(log_probs) * log_probs).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    loss = policy_loss - entropy_coef * entropy + outputs.loss\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395dd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# üîÑ RETRAINING FUNCTION\n",
    "# --------------------------\n",
    "def retrain_model():\n",
    "    \"\"\"Main retraining function with PPO\"\"\"\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "        \n",
    "        # Initialize reward model with fallback options\n",
    "        reward_model = RewardModel().to(DEVICE)\n",
    "        if os.path.exists(\"reward_model.pth\"):\n",
    "            try:\n",
    "                reward_model.load_state_dict(torch.load(\"reward_model.pth\", map_location=DEVICE))\n",
    "                logging.info(\"Successfully loaded reward model\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to load reward model: {str(e)}. Initializing new one.\")\n",
    "                reward_model.apply(self._init_weights)\n",
    "        else:\n",
    "            logging.warning(\"No reward_model.pth found. Initializing new reward model.\")\n",
    "            reward_model.apply(self._init_weights)\n",
    "            # Optionally train a basic reward model first\n",
    "            if len(pd.read_csv(FEEDBACK_CSV)) >= FEEDBACK_THRESHOLD:\n",
    "                logging.info(\"Training initial reward model...\")\n",
    "                train_basic_reward_model()  # Implement this function\n",
    "        \n",
    "        # Load feedback data\n",
    "        dataset = load_feedback_data(tokenizer)\n",
    "        if dataset is None:\n",
    "            return False\n",
    "            \n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=100,\n",
    "            num_training_steps=len(dataloader) * EPOCHS\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        reward_model.train()  # Allow reward model to learn during training\n",
    "        logging.info(\"Starting retraining process...\")\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "                \n",
    "                # PPO training step with error handling\n",
    "                try:\n",
    "                    loss = ppo_train_step(\n",
    "                        model,\n",
    "                        batch,\n",
    "                        reward_model,\n",
    "                        optimizer,\n",
    "                        PPO_CLIP,\n",
    "                        ENTROPY_COEF\n",
    "                    )\n",
    "                    epoch_loss += loss\n",
    "                    progress_bar.set_postfix(loss=loss)\n",
    "                    scheduler.step()\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error in batch processing: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save intermediate checkpoints\n",
    "            if (epoch + 1) % 2 == 0:  # Save every 2 epochs\n",
    "                torch.save(reward_model.state_dict(), f\"reward_model_epoch_{epoch+1}.pth\")\n",
    "            \n",
    "            logging.info(f\"Epoch {epoch+1} completed. Avg loss: {epoch_loss/len(dataloader)}\")\n",
    "        \n",
    "        # Save final models\n",
    "        model.save_pretrained(RETRAINED_MODEL_PATH)\n",
    "        tokenizer.save_pretrained(RETRAINED_MODEL_PATH)\n",
    "        torch.save(reward_model.state_dict(), \"reward_model.pth\")\n",
    "        \n",
    "        # Archive feedback data\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        archive_dir = Path(\"feedback_data/archive\")\n",
    "        archive_dir.mkdir(exist_ok=True)\n",
    "        os.rename(FEEDBACK_CSV, archive_dir / f\"feedback_{timestamp}.csv\")\n",
    "        \n",
    "        logging.info(\"Retraining completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Retraining failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def _init_weights(self, module):\n",
    "    \"\"\"Initialize weights for the reward model\"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "def train_basic_reward_model():\n",
    "    \"\"\"Train a simple initial reward model if none exists\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(FEEDBACK_CSV)\n",
    "        if len(df) < 10:  # Minimum samples needed\n",
    "            return False\n",
    "            \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        reward_model = RewardModel().to(DEVICE)\n",
    "        optimizer = AdamW(reward_model.parameters(), lr=1e-4)\n",
    "        \n",
    "        # Simple training loop\n",
    "        for epoch in range(3):  # Fewer epochs for initial training\n",
    "            for _, row in df.iterrows():\n",
    "                inputs = tokenizer(\n",
    "                    f\"{row['article']} [SEP] {row['summary']}\", \n",
    "                    return_tensors='pt'\n",
    "                ).to(DEVICE)\n",
    "                target = torch.tensor([row['feedback']], dtype=torch.float).to(DEVICE)\n",
    "                \n",
    "                outputs = reward_model(inputs['input_ids'])\n",
    "                loss = torch.nn.functional.binary_cross_entropy(outputs, target)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        torch.save(reward_model.state_dict(), \"reward_model.pth\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Basic reward model training failed: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767957c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CIS_Project\\venv\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Retraining failed. Check logs for details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# üöÄ AUTOMATION ENTRY POINT\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if retraining is needed\n",
    "    if os.path.exists(FEEDBACK_CSV):\n",
    "        df = pd.read_csv(FEEDBACK_CSV)\n",
    "        positive_feedbacks = len(df[df['feedback'] == 1])\n",
    "        \n",
    "        if positive_feedbacks >= FEEDBACK_THRESHOLD:\n",
    "            logging.info(f\"Starting retraining with {positive_feedbacks} positive feedbacks\")\n",
    "            success = retrain_model()\n",
    "            \n",
    "            if success:\n",
    "                print(\"‚úÖ Retraining completed successfully!\")\n",
    "            else:\n",
    "                print(\"‚ùå Retraining failed. Check logs for details.\")\n",
    "        else:\n",
    "            print(f\"Not enough feedbacks ({positive_feedbacks}/{FEEDBACK_THRESHOLD})\")\n",
    "    else:\n",
    "        print(\"No feedback data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fcf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting retraining with 15 positive feedbacks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CIS_Project\\venv\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Epoch 1/3:   0%|          | 0/4 [00:00<?, ?it/s]e:\\CIS_Project\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # This is the missing import\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# --------------------------\n",
    "# üõ†Ô∏è CONFIGURATION\n",
    "# --------------------------\n",
    "MODEL_PATH = \"bart_summarizer_with_rl\"\n",
    "FEEDBACK_CSV = \"feedback_data/feedback_log.csv\"\n",
    "RETRAINED_MODEL_PATH = \"bart_summarizer_with_rl_retrained\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "PPO_CLIP = 0.2\n",
    "ENTROPY_COEF = 0.01\n",
    "FEEDBACK_THRESHOLD = 10\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='retraining.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# üß† REWARD MODEL\n",
    "# --------------------------\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.fc2(x))\n",
    "\n",
    "# --------------------------\n",
    "# üîÑ RETRAINING UTILITIES\n",
    "# --------------------------\n",
    "def _init_weights(module):\n",
    "    \"\"\"Initialize weights for the reward model\"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "def train_basic_reward_model():\n",
    "    \"\"\"Train a simple initial reward model if none exists\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(FEEDBACK_CSV)\n",
    "        if len(df) < 10:  # Minimum samples needed\n",
    "            return False\n",
    "            \n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        reward_model = RewardModel().to(DEVICE)\n",
    "        optimizer = AdamW(reward_model.parameters(), lr=1e-4)\n",
    "        \n",
    "        # Simple training loop\n",
    "        for epoch in range(3):  # Fewer epochs for initial training\n",
    "            for _, row in df.iterrows():\n",
    "                inputs = tokenizer(\n",
    "                    f\"{row['article']} [SEP] {row['summary']}\", \n",
    "                    return_tensors='pt'\n",
    "                ).to(DEVICE)\n",
    "                target = torch.tensor([row['feedback']], dtype=torch.float).to(DEVICE)\n",
    "                \n",
    "                outputs = reward_model(inputs['input_ids'].float())\n",
    "                loss = F.binary_cross_entropy(outputs, target)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        torch.save(reward_model.state_dict(), \"reward_model.pth\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Basic reward model training failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# --------------------------\n",
    "# üîÑ RETRAINING FUNCTION\n",
    "# --------------------------\n",
    "def retrain_model():\n",
    "    \"\"\"Main retraining function with PPO\"\"\"\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "        \n",
    "        # Initialize reward model with proper weight initialization\n",
    "        reward_model = RewardModel().to(DEVICE)\n",
    "        if os.path.exists(\"reward_model.pth\"):\n",
    "            try:\n",
    "                reward_model.load_state_dict(torch.load(\"reward_model.pth\", map_location=DEVICE))\n",
    "                logging.info(\"Successfully loaded reward model\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to load reward model: {str(e)}. Initializing new one.\")\n",
    "                reward_model.apply(_init_weights)\n",
    "        else:\n",
    "            logging.warning(\"No reward_model.pth found. Initializing new reward model.\")\n",
    "            reward_model.apply(_init_weights)\n",
    "            train_basic_reward_model()  # Train initial reward model\n",
    "        \n",
    "        # Load feedback data\n",
    "        if not os.path.exists(FEEDBACK_CSV):\n",
    "            logging.error(\"Feedback file not found\")\n",
    "            return False\n",
    "            \n",
    "        df = pd.read_csv(FEEDBACK_CSV)\n",
    "        positive_feedback = df[df['feedback'] == 1]\n",
    "        if len(positive_feedback) < FEEDBACK_THRESHOLD:\n",
    "            logging.info(f\"Not enough positive feedbacks ({len(positive_feedback)}/{FEEDBACK_THRESHOLD})\")\n",
    "            return False\n",
    "            \n",
    "        # Prepare dataloader\n",
    "        dataset = FeedbackDataset(tokenizer, positive_feedback)\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=100,\n",
    "            num_training_steps=len(dataloader) * EPOCHS\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        reward_model.train()\n",
    "        logging.info(\"Starting retraining process...\")\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "                \n",
    "                try:\n",
    "                    # PPO training step would go here\n",
    "                    # For now using basic training\n",
    "                    outputs = model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels']\n",
    "                    )\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    epoch_loss += loss.item()\n",
    "                    progress_bar.set_postfix(loss=loss.item())\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error in batch processing: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save checkpoint\n",
    "            torch.save(reward_model.state_dict(), f\"reward_model_epoch_{epoch+1}.pth\")\n",
    "            logging.info(f\"Epoch {epoch+1} completed. Avg loss: {epoch_loss/len(dataloader)}\")\n",
    "        \n",
    "        # Save final models\n",
    "        model.save_pretrained(RETRAINED_MODEL_PATH)\n",
    "        tokenizer.save_pretrained(RETRAINED_MODEL_PATH)\n",
    "        torch.save(reward_model.state_dict(), \"reward_model.pth\")\n",
    "        \n",
    "        # Archive feedback data\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        archive_dir = Path(\"feedback_data/archive\")\n",
    "        archive_dir.mkdir(exist_ok=True)\n",
    "        os.rename(FEEDBACK_CSV, archive_dir / f\"feedback_{timestamp}.csv\")\n",
    "        \n",
    "        logging.info(\"Retraining completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Retraining failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists(FEEDBACK_CSV):\n",
    "        df = pd.read_csv(FEEDBACK_CSV)\n",
    "        positive_count = len(df[df['feedback'] == 1])\n",
    "        \n",
    "        if positive_count >= FEEDBACK_THRESHOLD:\n",
    "            print(f\"Starting retraining with {positive_count} positive feedbacks\")\n",
    "            if retrain_model():\n",
    "                print(\"‚úÖ Retraining successful!\")\n",
    "            else:\n",
    "                print(\"‚ùå Retraining failed - check retraining.log\")\n",
    "        else:\n",
    "            print(f\"Need {FEEDBACK_THRESHOLD} positive feedbacks (have {positive_count})\")\n",
    "    else:\n",
    "        print(\"No feedback data found at:\", FEEDBACK_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf18fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
